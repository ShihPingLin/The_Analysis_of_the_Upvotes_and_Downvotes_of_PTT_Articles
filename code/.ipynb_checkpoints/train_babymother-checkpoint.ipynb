{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:25:23.714673Z",
     "start_time": "2020-01-03T07:25:23.008141Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = open('babymother_max.csv')\n",
    "df= pd.read_csv(f, header = None)\n",
    "df = df.fillna('0')\n",
    "\n",
    "X = df.iloc[:, 2].values\n",
    "y = df.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:25:23.958154Z",
     "start_time": "2020-01-03T07:25:23.717637Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing : extracting feature vector from X_train\n",
    "# 拿掉換行和標點符號\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "import string\n",
    "\n",
    "eng_punc = string.punctuation\n",
    "def preprocessor(text):\n",
    "    text = re.sub('\\n', '', text)\n",
    "    \n",
    "    re_punctuation = \"[{}]+\".format(punctuation)  #拿掉中文標點\n",
    "    text = re.sub(re_punctuation, \"\", text)\n",
    "    \n",
    "    re_punctuation_eng = \"[{}]+\".format(eng_punc) #拿掉英文標點\n",
    "    text = re.sub(re_punctuation_eng, \"\", text)    \n",
    "    \n",
    "    ptt_punc = '\\n\\t ※'\n",
    "    re_punctuation_ptt = \"[{}]+\".format(ptt_punc) #拿掉其他無用字元\n",
    "    text = re.sub(re_punctuation_ptt, \"\", text)  \n",
    "    \n",
    "    text = re.sub('[0-9A-Za-z]+','',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    X[i] = preprocessor(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:25:48.603717Z",
     "start_time": "2020-01-03T07:25:23.961149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\User\\Desktop\\pytry\\final\\FFF\\jieba_dict\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.u3b6a00fb4c8494626298b2e460bd6669.cache\n",
      "Loading model cost 1.327 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing : tf-idf using jieba\n",
    "# preprocessing : 移除stop word\n",
    "import jieba.analyse\n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary(\"jieba_dict/dict.txt.big.txt\")\n",
    "tags = []\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    tags.append(jieba.analyse.extract_tags(X[i], topK=50, withWeight=True))\n",
    "stopWords = []\n",
    "with open('stop_word/stop_word.txt', 'r', encoding='UTF-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopWords.append(data)\n",
    "        \n",
    "X_cut = X\n",
    "X_cut_stop = X\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    X_cut[i] = jieba.cut(X[i], cut_all=False)\n",
    "    X_cut_stop[i] = list(filter(lambda a: a not in stopWords and a != '\\n', X_cut[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:25:48.628674Z",
     "start_time": "2020-01-03T07:25:48.606725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\nfor i in range(0, len(y)):\\n    if y[i] == '爆':\\n        y[i] = 1\\n    elif y[i][0] == 'X':\\n        y[i] = 0\\n    else:\\n        y[i] = int(y[i])\\n        if y[i] == 0:\\n            y[i] = 0\\n        elif y[i] >= 1 and y[i] <= 10:\\n            y[i] = 0\\n        elif y[i] >= 11 and y[i] <= 50:\\n            y[i] = 1\\n        elif y[i] >= 51 and y[i] <= 99:\\n            y[i] = 1\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing : labeling y_train\n",
    "# encoding of 推噓 : \n",
    "import math\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == '爆':\n",
    "        y[i] = 3\n",
    "    elif y[i][0] == 'X':\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = int(y[i])\n",
    "        if y[i] == 0:\n",
    "            y[i] = 1\n",
    "        elif y[i] >= 1 and y[i] <= 10:\n",
    "            y[i] = 1\n",
    "        elif y[i] >= 11 and y[i] <= 50:\n",
    "            y[i] = 2\n",
    "        elif y[i] >= 51 and y[i] <= 99:\n",
    "            y[i] = 3\n",
    "'''    \n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == '爆':\n",
    "        y[i] = 1\n",
    "    elif y[i][0] == 'X':\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = int(y[i])\n",
    "        if y[i] == 0:\n",
    "            y[i] = 0\n",
    "        elif y[i] >= 1 and y[i] <= 10:\n",
    "            y[i] = 0\n",
    "        elif y[i] >= 11 and y[i] <= 50:\n",
    "            y[i] = 1\n",
    "        elif y[i] >= 51 and y[i] <= 99:\n",
    "            y[i] = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:29:28.521721Z",
     "start_time": "2020-01-03T07:25:48.632643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing : 轉換成NTLK可以使用的格式\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "X_cut_NTLK = []\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    article = ''\n",
    "    for word in X_cut_stop[i]:\n",
    "        article = article + word\n",
    "        article = article + ' '\n",
    "    X_cut_NTLK = np.hstack((X_cut_NTLK, article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:29:30.875041Z",
     "start_time": "2020-01-03T07:29:28.528699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing : Bag of Word\n",
    "# Preprocessing : Term frequency - inverse document frequency (tf - idf) using NTLK\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(max_features=1000)\n",
    "bag = count.fit_transform(X_cut_NTLK)\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf = True,\n",
    "                         norm = 'l2',\n",
    "                         smooth_idf = True)\n",
    "X_bag_idf = tfidf.fit_transform(bag)\n",
    "print(X_bag_idf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:29:30.900972Z",
     "start_time": "2020-01-03T07:29:30.887007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3958\n",
      "3958\n"
     ]
    }
   ],
   "source": [
    "print(y.shape[0])\n",
    "print(X_bag_idf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:29:30.940867Z",
     "start_time": "2020-01-03T07:29:30.906954Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [  11 1781 1816  350]\n",
      "Labels counts in y_train: [   8 1246 1271  245]\n",
      "Labels counts in y_test: [  3 535 545 105]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bag_idf, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:29:31.262544Z",
     "start_time": "2020-01-03T07:29:30.944852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770\n",
      "1188\n",
      "3958\n",
      "3958\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.toarray()[:,0]))\n",
    "print(len(X_test.toarray()[:,0]))\n",
    "\n",
    "X_all = np.concatenate((X_train.toarray(),X_test.toarray()),axis=0)\n",
    "y_all = np.concatenate((y_train,y_test))\n",
    "\n",
    "print(len(X_all[:,0]))\n",
    "print(len(y_all))\n",
    "\n",
    "group = np.zeros(len(y_all), dtype=np.int)\n",
    "group[0:len(X_train.toarray()[:,0])] = group[0:len(X_train.toarray()[:,0])]+1\n",
    "df_re = pd.DataFrame(X_all)\n",
    "df_re['label'] = y_all\n",
    "df_re['group'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:51:47.150954Z",
     "start_time": "2020-01-03T07:51:46.901635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label 分布: [1271 1271 1271 1271]\n",
      "test label 分布: [  3 535 545 105]\n",
      "評分標準(只猜一群): 0.45875420875420875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_0 = df_re[df_re.group==1][df_re.label==0]\n",
    "df_1 = df_re[df_re.group==1][df_re.label==1]\n",
    "df_2 = df_re[df_re.group==1][df_re.label==2]\n",
    "df_3 = df_re[df_re.group==1][df_re.label==3]\n",
    "\n",
    "df_0 = resample(df_0, replace=True, n_samples=np.bincount(y_train).max(), random_state=123)\n",
    "df_1 = resample(df_1, replace=True, n_samples=np.bincount(y_train).max(), random_state=123)\n",
    "df_2 = resample(df_2, replace=True, n_samples=np.bincount(y_train).max(), random_state=123)\n",
    "df_3 = resample(df_3, replace=True, n_samples=np.bincount(y_train).max(), random_state=123)\n",
    "\n",
    "df_upsampled = pd.concat([df_0, df_1,df_2, df_3])\n",
    "\n",
    "X_train_up = np.asarray(df_upsampled.iloc[:,0:1000])\n",
    "y_train_up = np.asarray(df_upsampled.iloc[:,1000])\n",
    "X_test_up = np.asarray(df_re[df_re.group==0].iloc[:,0:1000])\n",
    "y_test_up = np.asarray(df_re[df_re.group==0].iloc[:,1000])\n",
    "\n",
    "print('train label 分布:',np.bincount(y_train_up))\n",
    "print('test label 分布:',np.bincount(y_test_up))\n",
    "print('評分標準(只猜一群):',len(y_test_up[y_test_up == 2])/len(y_test_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:06:06.159578Z",
     "start_time": "2019-12-30T09:06:05.747556Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 715\n",
      "Accuracy: 0.39\n",
      "Accuracy: 0.39\n",
      "\n",
      "test --- train\n",
      "\n",
      "Misclassified samples: 399\n",
      "Accuracy: 0.91\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C = 10, random_state = 1)\n",
    "lr.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred = lr.predict(X_test_up)\n",
    "print('Misclassified samples: %d' % (y_test_up != y_pred).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_test_up, y_pred))\n",
    "print('Accuracy: %.2f' %lr.score(X_test_up, y_test_up))\n",
    "\n",
    "print('\\ntest --- train\\n')\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred2 = lr.predict(X_train_up)\n",
    "print('Misclassified samples: %d' % (y_train_up != y_pred2).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_train_up, y_pred2))\n",
    "print('Accuracy: %.2f' %lr.score(X_train_up, y_train_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:06:06.171537Z",
     "start_time": "2019-12-30T09:06:06.161567Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 405 398 319]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:06:06.531841Z",
     "start_time": "2019-12-30T09:06:06.524861Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3190104166666667\n"
     ]
    }
   ],
   "source": [
    "j = y_pred[y_pred != 1]\n",
    "k = y_test_up[y_pred != 1]\n",
    "\n",
    "print(sum(j==k)/(len(j)+1))  ## 小群的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:07:02.996605Z",
     "start_time": "2019-12-30T09:06:07.059178Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 675\n",
      "Accuracy: 0.42\n",
      "Accuracy: 0.42\n",
      "\n",
      "test --- train\n",
      "\n",
      "Misclassified samples: 13\n",
      "Accuracy: 1.00\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# training with SVM(RBF)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel = 'rbf', C = 100, gamma = 1000, random_state = 1)\n",
    "svm.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred = svm.predict(X_test_up)\n",
    "print('Misclassified samples: %d' % (y_test_up != y_pred).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_test_up, y_pred))\n",
    "print('Accuracy: %.2f' %svm.score(X_test_up, y_test_up))\n",
    "\n",
    "print('\\ntest --- train\\n')\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred2 = svm.predict(X_train_up)\n",
    "print('Misclassified samples: %d' % (y_train_up != y_pred2).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_train_up, y_pred2))\n",
    "print('Accuracy: %.2f' %svm.score(X_train_up, y_train_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:07:03.011564Z",
     "start_time": "2019-12-30T09:07:03.000588Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 1165    0    7]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:07:03.027517Z",
     "start_time": "2019-12-30T09:07:03.016577Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "j = y_pred[y_pred != 1]\n",
    "k = y_test_up[y_pred != 1]\n",
    "\n",
    "print(sum(j==k)/(len(j)+1))  ## 小群的正確率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging vs boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:52:15.941056Z",
     "start_time": "2020-01-03T07:52:05.958292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 520\n",
      "Accuracy: 0.56\n",
      "Accuracy: 0.56\n",
      "\n",
      "test --- train\n",
      "\n",
      "Misclassified samples: 2\n",
      "Accuracy: 1.00\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# training with forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=500, random_state=1, n_jobs=-1)\n",
    "forest.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred = forest.predict(X_test_up)\n",
    "print('Misclassified samples: %d' % (y_test_up != y_pred).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_test_up, y_pred))\n",
    "print('Accuracy: %.2f' %forest.score(X_test_up, y_test_up))\n",
    "\n",
    "print('\\ntest --- train\\n')\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred2 = forest.predict(X_train_up)\n",
    "print('Misclassified samples: %d' % (y_train_up != y_pred2).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_train_up, y_pred2))\n",
    "print('Accuracy: %.2f' %forest.score(X_train_up, y_train_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:52:15.950032Z",
     "start_time": "2020-01-03T07:52:15.944047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 511 633  44]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T07:52:15.965989Z",
     "start_time": "2020-01-03T07:52:15.958010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5701438848920863\n"
     ]
    }
   ],
   "source": [
    "j = y_pred[y_pred != 2]\n",
    "k = y_test_up[y_pred != 2]\n",
    "\n",
    "print(sum(j==k)/(len(j)+1))  ## 小群的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:58:51.326542Z",
     "start_time": "2020-01-03T08:58:38.543346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 575\n",
      "Accuracy: 0.52\n",
      "Accuracy: 0.52\n",
      "\n",
      "test --- train\n",
      "\n",
      "Misclassified samples: 1877\n",
      "Accuracy: 0.63\n",
      "Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=1)\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=30, learning_rate=0.1, random_state=1)\n",
    "ada = ada.fit(X_train_up,y_train_up)\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred = ada.predict(X_test_up)\n",
    "print('Misclassified samples: %d' % (y_test_up != y_pred).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_test_up, y_pred))\n",
    "print('Accuracy: %.2f' %ada.score(X_test_up, y_test_up))\n",
    "\n",
    "print('\\ntest --- train\\n')\n",
    "\n",
    "# Prediction and Performance Measurement\n",
    "y_pred2 = ada.predict(X_train_up)\n",
    "print('Misclassified samples: %d' % (y_train_up != y_pred2).sum())\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' %accuracy_score(y_train_up, y_pred2))\n",
    "print('Accuracy: %.2f' %ada.score(X_train_up, y_train_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:58:51.336501Z",
     "start_time": "2020-01-03T08:58:51.329520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 758 387  43]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:58:51.352470Z",
     "start_time": "2020-01-03T08:58:51.343490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "j = y_pred[y_pred != 2]\n",
    "k = y_test_up[y_pred != 2]\n",
    "\n",
    "print(sum(j==k)/(len(j)+1))  ## 小群的正確率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
